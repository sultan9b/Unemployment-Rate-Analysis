import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "sonador"
DAG_ID = "raw_unemployment_from_owid_to_s3"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"
SOURCE = "unemployment_owid"

# S3 / MinIO
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° Our World in Data
OWID_DATASET = "unemployment-rate"

# Ğ¡Ñ‚Ñ€Ğ°Ğ½Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ»ÑĞ±Ñ‹Ğµ, Ñ€Ğ°Ğ·Ğ´ĞµĞ»ÑÑ ~)
# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹: USA, CHN, IND, RUS, GBR, DEU, FRA, JPN, BRA, KAZ
COUNTRIES = "USA~CHN~IND~RUS~GBR~DEU~FRA~JPN~BRA~KAZ"

LONG_DESCRIPTION = """
# Unemployment Rate Data Pipeline

Ğ­Ñ‚Ğ¾Ñ‚ DAG Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ± ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Our World in Data 
Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² MinIO/S3 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet.

## Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: Our World in Data
- Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚: Unemployment Rate
- ĞŸĞµÑ€Ğ²Ğ¾Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: International Labour Organization (ILO)
- Ğ¡Ñ‚Ñ€Ğ°Ğ½Ñ‹: Ğ¡Ğ¨Ğ, ĞšĞ¸Ñ‚Ğ°Ğ¹, Ğ˜Ğ½Ğ´Ğ¸Ñ, Ğ Ğ¾ÑÑĞ¸Ñ, Ğ’ĞµĞ»Ğ¸ĞºĞ¾Ğ±Ñ€Ğ¸Ñ‚Ğ°Ğ½Ğ¸Ñ, Ğ“ĞµÑ€Ğ¼Ğ°Ğ½Ğ¸Ñ, Ğ¤Ñ€Ğ°Ğ½Ñ†Ğ¸Ñ, Ğ¯Ğ¿Ğ¾Ğ½Ğ¸Ñ, Ğ‘Ñ€Ğ°Ğ·Ğ¸Ğ»Ğ¸Ñ, ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½

## ĞŸĞ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Entity: Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹
- Code: ĞºĞ¾Ğ´ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹ (ISO)
- Year: Ğ³Ğ¾Ğ´
- Unemployment rate (%): ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°Ñ…

## Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Parquet (ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ GZIP)
- Ğ Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ: s3://prod/raw/unemployment_owid/YYYY-MM-DD/

## ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°
- âœ… ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ±ĞµÑĞ¿Ğ»Ğ°Ñ‚Ğ½Ğ¾
- âœ… Ğ‘ĞµĞ· API ĞºĞ»ÑÑ‡Ğ°
- âœ… Ğ‘ĞµĞ· Ğ»Ğ¸Ğ¼Ğ¸Ñ‚Ğ¾Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
- âœ… ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ñ‚ ILO
- âœ… Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ 1991 Ğ³Ğ¾Ğ´Ğ°
"""

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Our World in Data Ğ² S3"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(1991, 1, 1, tz="Asia/Almaty"),
    "catchup": False,  # ĞĞµ Ğ½ÑƒĞ¶ĞµĞ½ catchup - Ğ²ÑĞµĞ³Ğ´Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Airflow"""
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ± ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ¸Ğ· Our World in Data CSV API
    Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² MinIO/S3 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet
    """

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for date: {start_date}")

    # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ URL Ğ´Ğ»Ñ Our World in Data CSV API
    csv_url = (
        f"https://ourworldindata.org/grapher/{OWID_DATASET}.csv"
        f"?country={COUNTRIES}"
    )

    logging.info(f"ğŸ“¡ Fetching unemployment data from Our World in Data")
    logging.info(f"ğŸ”— CSV URL: {csv_url}")

    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº DuckDB
    con = duckdb.connect()

    try:
        # ĞŸÑƒÑ‚ÑŒ Ğ² S3
        s3_path = f"s3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet"

        logging.info(f"ğŸ“¤ Processing and uploading data to S3: {s3_path}")

        # DuckDB Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ CSV Ğ¸Ğ· OWID Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² S3
        con.sql(f"""
            SET TIMEZONE='UTC';
            INSTALL httpfs;
            LOAD httpfs;
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;

            COPY (
                SELECT
                    '{start_date}' as load_date,
                    *
                FROM read_csv_auto('{csv_url}')
            ) TO '{s3_path}'
            (FORMAT PARQUET, COMPRESSION GZIP);
        """)

        logging.info(f"âœ… Data successfully saved to S3: {s3_path}")

        # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        result = con.sql(f"""
            SELECT 
                COUNT(*) as total_rows,
                COUNT(DISTINCT Entity) as countries,
                MIN(Year) as min_year,
                MAX(Year) as max_year,
                ROUND(AVG("Unemployment, total (% of total labor force) (modeled ILO estimate)"), 2) as avg_unemployment
            FROM read_csv_auto('{csv_url}')
        """).fetchone()

        logging.info(f"ğŸ“Š Unemployment Data Statistics:")
        logging.info(f"   Total rows: {result[0]}")
        logging.info(f"   Countries: {result[1]}")
        logging.info(f"   Year range: {result[2]} - {result[3]}")
        logging.info(f"   Average unemployment rate: {result[4]}%")

        # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğµ
        latest_data = con.sql(f"""
            SELECT 
                Entity,
                Year,
                ROUND("Unemployment, total (% of total labor force) (modeled ILO estimate)", 2) as unemployment_rate
            FROM read_csv_auto('{csv_url}')
            WHERE Year = (SELECT MAX(Year) FROM read_csv_auto('{csv_url}'))
            ORDER BY unemployment_rate DESC
        """).fetchall()

        logging.info(f"ğŸ“ˆ Latest unemployment rates:")
        for row in latest_data:
            logging.info(f"   {row[0]}: {row[2]}% ({row[1]})")

    except Exception as e:
        logging.error(f"âŒ Error processing data: {str(e)}")
        raise
    finally:
        con.close()

    logging.info(f"âœ… Download for date success: {start_date}")


# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ DAG
with DAG(
        dag_id=DAG_ID,
        schedule_interval="@monthly",  # Ğ Ğ°Ğ· Ğ² Ğ¼ĞµÑÑÑ† (Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑÑÑ‚ÑÑ Ñ€ĞµĞ´ĞºĞ¾)
        default_args=args,
        tags=["s3", "raw", "unemployment", "owid", "ilo", "economics"],
        description=SHORT_DESCRIPTION,
        concurrency=1,
        max_active_tasks=1,
        max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id="start",
    )

    get_and_transfer_api_data_to_s3_task = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> get_and_transfer_api_data_to_s3_task >> end