import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "sonador"
DAG_ID = "raw_from_api_to_s3"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"
SOURCE = "spy_etf"

# S3 / MinIO
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# Alpha Vantage API
ALPHA_VANTAGE_API_KEY = Variable.get("alpha_vantage_api_key")  # Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ² Airflow Variables

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ SPY ETF
SYMBOL = "SPY"  # S&P 500 ETF
OUTPUT_SIZE = "full"  # "compact" (100 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ…) Ğ¸Ğ»Ğ¸ "full" (20+ Ğ»ĞµÑ‚)

LONG_DESCRIPTION = """
# SPY ETF Daily Stock Data Pipeline

Ğ­Ñ‚Ğ¾Ñ‚ DAG Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ SPY ETF (S&P 500) 
Ğ¸Ğ· Alpha Vantage API Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² MinIO/S3 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet.

## Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- API: Alpha Vantage Time Series Daily Adjusted
- Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ»: SPY (SPDR S&P 500 ETF Trust)
- Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: OHLCV + adjusted close + dividend + split
- Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ: Ğ¿Ğ¾Ğ»Ğ½Ğ°Ñ (20+ Ğ»ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)

## ĞŸĞ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- timestamp: Ğ´Ğ°Ñ‚Ğ° Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²
- open: Ñ†ĞµĞ½Ğ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ
- high: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°
- low: Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ†ĞµĞ½Ğ°  
- close: Ñ†ĞµĞ½Ğ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ
- adjusted_close: ÑĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ†ĞµĞ½Ğ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ
- volume: Ğ¾Ğ±ÑŠĞµĞ¼ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²
- dividend_amount: Ğ´Ğ¸Ğ²Ğ¸Ğ´ĞµĞ½Ğ´Ñ‹
- split_coefficient: ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ ÑĞ¿Ğ»Ğ¸Ñ‚Ğ°

## Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Parquet (ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ GZIP)
- Ğ Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ: s3://prod/raw/spy_etf/YYYY-MM-DD/

## ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸Ñ API
- Free tier: 25 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ´ĞµĞ½ÑŒ
- Premium: 75+ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ² Ğ´ĞµĞ½ÑŒ
"""

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° ĞºĞ¾Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº SPY ETF Ğ¸Ğ· Alpha Vantage API Ğ² S3"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 1, 1, tz="Asia/Almaty"),
    "catchup": False,  # ĞĞµ Ğ½ÑƒĞ¶ĞµĞ½ catchup - ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ€Ğ°Ğ· Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Airflow"""
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date


def get_and_transfer_api_data_to_s3(**context):
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ SPY ETF Ğ¸Ğ· Alpha Vantage API
    Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ² MinIO/S3 Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Parquet
    """

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for date: {start_date}")

    # ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ URL Ğ´Ğ»Ñ Alpha Vantage API
    api_url = (
        f"https://www.alphavantage.co/query"
        f"?function=TIME_SERIES_DAILY"
        f"&symbol={SYMBOL}"
        f"&outputsize={OUTPUT_SIZE}"
        f"&apikey={ALPHA_VANTAGE_API_KEY}"
        f"&datatype=csv"
    )

    logging.info(f"ğŸ“¡ Fetching {SYMBOL} ETF data from Alpha Vantage API")
    logging.info(f"ğŸ”— API URL: {api_url.replace(ALPHA_VANTAGE_API_KEY, '***')}")

    # ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒÑÑ Ğº DuckDB
    con = duckdb.connect()

    try:
        # ĞŸÑƒÑ‚ÑŒ Ğ² S3
        s3_path = f"s3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet"

        logging.info(f"ğŸ“¤ Processing and uploading data to S3: {s3_path}")

        # DuckDB Ñ‡Ğ¸Ñ‚Ğ°ĞµÑ‚ CSV Ğ¸Ğ· Alpha Vantage Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ² S3
        con.sql(f"""
            SET TIMEZONE='UTC';
            INSTALL httpfs;
            LOAD httpfs;
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;

            COPY (
                SELECT
                    '{start_date}' as load_date,
                    '{SYMBOL}' as symbol,
                    timestamp,
                    open,
                    high,
                    low,
                    close,
                    volume
                FROM read_csv_auto('{api_url}')
            ) TO '{s3_path}'
            (FORMAT PARQUET, COMPRESSION GZIP);
        """)

        logging.info(f"âœ… Data successfully saved to S3: {s3_path}")

        # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        result = con.sql(f"""
            SELECT 
                COUNT(*) as total_rows,
                MIN(timestamp) as earliest_date,
                MAX(timestamp) as latest_date,
                ROUND(AVG(close), 2) as avg_close_price,
                ROUND(MIN(close), 2) as min_close_price,
                ROUND(MAX(close), 2) as max_close_price,
                ROUND(SUM(volume), 0) as total_volume
            FROM read_csv_auto('{api_url}')
        """).fetchone()

        logging.info(f"ğŸ“Š {SYMBOL} ETF Data Statistics:")
        logging.info(f"   Total trading days: {result[0]}")
        logging.info(f"   Date range: {result[1]} to {result[2]}")
        logging.info(f"   Average close price: ${result[3]}")
        logging.info(f"   Price range: ${result[4]} - ${result[5]}")
        logging.info(f"   Total volume: {result[6]:,.0f}")

    except Exception as e:
        logging.error(f"âŒ Error processing data: {str(e)}")

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        if "API call frequency" in str(e) or "premium" in str(e).lower():
            logging.error("âš ï¸ API rate limit exceeded. Free tier allows 25 requests/day.")
        elif "Invalid API call" in str(e):
            logging.error("âš ï¸ Check if API key is valid and set correctly in Airflow Variables.")

        raise
    finally:
        con.close()

    logging.info(f"âœ… Download for date success: {start_date}")


# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ DAG
with DAG(
        dag_id=DAG_ID,
        schedule_interval="0 2 * * *",  # ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ Ğ² 02:00 UTC (Ğ¿Ğ¾ÑĞ»Ğµ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ US Ñ€Ñ‹Ğ½ĞºĞ°)
        default_args=args,
        tags=["s3", "raw", "stocks", "spy", "etf", "alpha-vantage"],
        description=SHORT_DESCRIPTION,
        concurrency=1,
        max_active_tasks=1,
        max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id="start",
    )

    get_and_transfer_api_data_to_s3_task = PythonOperator(
        task_id="get_and_transfer_api_data_to_s3",
        python_callable=get_and_transfer_api_data_to_s3,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> get_and_transfer_api_data_to_s3_task >> end
